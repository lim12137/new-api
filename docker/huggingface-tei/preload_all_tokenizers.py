#!/usr/bin/env python3
"""
å¼ºåˆ¶é¢„ä¸‹è½½æ‰€æœ‰åˆ†è¯å™¨è„šæœ¬
ç¡®ä¿æ‰€æœ‰åˆ†è¯å™¨éƒ½ä¸‹è½½åˆ°æœ¬åœ°ï¼Œæ”¯æŒç¦»çº¿ä½¿ç”¨
"""

import os
import sys
import json
from transformers import AutoTokenizer
from huggingface_hub import snapshot_download

# è®¾ç½®ç¼“å­˜ç›®å½•
cache_dir = "/data/cache"
os.makedirs(cache_dir, exist_ok=True)

# å®Œæ•´çš„æ¨¡å‹åˆ—è¡¨ - åŒ…å«æ‰€æœ‰æ”¯æŒçš„é‡æ’åºå’ŒåµŒå…¥æ¨¡å‹
ALL_MODELS = [
    # BGE é‡æ’åºæ¨¡å‹
    "BAAI/bge-reranker-v2-m3",
    "BAAI/bge-reranker-large", 
    "BAAI/bge-reranker-base",
    "BAAI/bge-reranker-v2-gemma",
    "BAAI/bge-reranker-v2-minicpm-layerwise",
    
    # Jina é‡æ’åºæ¨¡å‹
    "jinaai/jina-reranker-v2-base-multilingual",
    "jinaai/jina-reranker-v1-base-en",
    "jinaai/jina-reranker-v1-turbo-en",
    "jinaai/jina-reranker-v1-tiny-en",
    
    # Cross-encoder é‡æ’åºæ¨¡å‹
    "cross-encoder/ms-marco-MiniLM-L-6-v2",
    "cross-encoder/ms-marco-MiniLM-L-12-v2", 
    "cross-encoder/ms-marco-TinyBERT-L-2-v2",
    "cross-encoder/ms-marco-electra-base",
    
    # Mixedbread AI é‡æ’åºæ¨¡å‹
    "mixedbread-ai/mxbai-rerank-large-v1",
    "mixedbread-ai/mxbai-rerank-base-v1",
    "mixedbread-ai/mxbai-rerank-xsmall-v1",
    
    # Sentence Transformers åµŒå…¥æ¨¡å‹
    "sentence-transformers/all-MiniLM-L6-v2",
    "sentence-transformers/all-MiniLM-L12-v2",
    "sentence-transformers/all-mpnet-base-v2",
    "sentence-transformers/all-distilroberta-v1",
    "sentence-transformers/paraphrase-MiniLM-L6-v2",
    
    # BGE åµŒå…¥æ¨¡å‹ - è‹±æ–‡
    "BAAI/bge-small-en-v1.5",
    "BAAI/bge-base-en-v1.5",
    "BAAI/bge-large-en-v1.5",
    "BAAI/bge-m3",
    
    # BGE åµŒå…¥æ¨¡å‹ - ä¸­æ–‡
    "BAAI/bge-small-zh-v1.5",
    "BAAI/bge-base-zh-v1.5",
    "BAAI/bge-large-zh-v1.5",
    
    # å…¶ä»–æµè¡Œçš„åµŒå…¥æ¨¡å‹
    "thenlper/gte-small",
    "thenlper/gte-base",
    "thenlper/gte-large",
    "intfloat/e5-small-v2",
    "intfloat/e5-base-v2",
    "intfloat/e5-large-v2",
]

def force_download_tokenizer(model_name):
    """å¼ºåˆ¶ä¸‹è½½æŒ‡å®šæ¨¡å‹çš„åˆ†è¯å™¨å’Œç›¸å…³æ–‡ä»¶"""
    print(f"ğŸ”„ å¼ºåˆ¶ä¸‹è½½æ¨¡å‹: {model_name}")
    
    success = False
    
    try:
        # æ–¹æ³•1: ä½¿ç”¨ snapshot_download ä¸‹è½½å®Œæ•´ä»“åº“
        print(f"  ğŸ“¦ ä¸‹è½½å®Œæ•´æ¨¡å‹ä»“åº“...")
        snapshot_download(
            repo_id=model_name,
            cache_dir=cache_dir,
            local_files_only=False,
            resume_download=True,
            force_download=False,  # é¿å…é‡å¤ä¸‹è½½
            ignore_patterns=["*.bin", "*.safetensors", "pytorch_model.bin"]  # è·³è¿‡å¤§çš„æƒé‡æ–‡ä»¶
        )
        
        # æ–¹æ³•2: ä¸“é—¨ä¸‹è½½åˆ†è¯å™¨
        print(f"  ğŸ”¤ ä¸‹è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True,
            use_fast=True,
            local_files_only=False,
            force_download=False
        )
        
        # éªŒè¯åˆ†è¯å™¨å·¥ä½œæ­£å¸¸
        test_texts = [
            "Hello, world!",
            "ä½ å¥½ä¸–ç•Œï¼",
            "This is a test sentence for tokenization.",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚"
        ]
        
        for test_text in test_texts:
            try:
                tokens = tokenizer.encode(test_text, add_special_tokens=True)
                decoded = tokenizer.decode(tokens, skip_special_tokens=True)
                if len(tokens) > 0:
                    print(f"  âœ… åˆ†è¯å™¨æµ‹è¯•é€šè¿‡: '{test_text}' -> {len(tokens)} tokens")
                    success = True
                    break
            except Exception as e:
                print(f"  âš ï¸  åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        if success:
            print(f"  âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {model_name}")
        else:
            print(f"  âŒ åˆ†è¯å™¨éªŒè¯å¤±è´¥: {model_name}")
            
    except Exception as e:
        print(f"  âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥ {model_name}: {e}")
        success = False
    
    return success

def verify_cache_structure():
    """éªŒè¯ç¼“å­˜ç›®å½•ç»“æ„"""
    print(f"\nğŸ“ éªŒè¯ç¼“å­˜ç›®å½•ç»“æ„: {cache_dir}")
    
    total_files = 0
    tokenizer_files = 0
    config_files = 0
    
    for root, dirs, files in os.walk(cache_dir):
        for file in files:
            total_files += 1
            if file in ['tokenizer.json', 'tokenizer_config.json', 'vocab.txt', 'vocab.json']:
                tokenizer_files += 1
            elif file in ['config.json', 'tokenizer_config.json']:
                config_files += 1
    
    print(f"  ğŸ“Š æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"  ğŸ”¤ åˆ†è¯å™¨æ–‡ä»¶æ•°: {tokenizer_files}")
    print(f"  âš™ï¸  é…ç½®æ–‡ä»¶æ•°: {config_files}")
    
    return tokenizer_files > 0

def create_offline_config():
    """åˆ›å»ºç¦»çº¿ä½¿ç”¨é…ç½®"""
    config_file = os.path.join(cache_dir, "offline_config.json")
    
    config = {
        "offline_mode": True,
        "cache_dir": cache_dir,
        "downloaded_models": [],
        "download_timestamp": None
    }
    
    # è®°å½•å·²ä¸‹è½½çš„æ¨¡å‹
    for model in ALL_MODELS:
        model_cache_path = os.path.join(cache_dir, f"models--{model.replace('/', '--')}")
        if os.path.exists(model_cache_path):
            config["downloaded_models"].append(model)
    
    import time
    config["download_timestamp"] = time.time()
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"  ğŸ“ ç¦»çº¿é…ç½®å·²ä¿å­˜: {config_file}")
    print(f"  ğŸ“¦ å·²ä¸‹è½½æ¨¡å‹æ•°: {len(config['downloaded_models'])}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¼ºåˆ¶é¢„ä¸‹è½½æ‰€æœ‰åˆ†è¯å™¨...")
    print(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
    print(f"ğŸ“‹ æ¨¡å‹æ€»æ•°: {len(ALL_MODELS)}")
    
    success_count = 0
    failed_models = []
    
    for i, model in enumerate(ALL_MODELS, 1):
        print(f"\n[{i}/{len(ALL_MODELS)}] å¤„ç†æ¨¡å‹: {model}")
        
        if force_download_tokenizer(model):
            success_count += 1
        else:
            failed_models.append(model)
    
    print(f"\nğŸ¯ ä¸‹è½½å®Œæˆç»Ÿè®¡:")
    print(f"  âœ… æˆåŠŸ: {success_count}/{len(ALL_MODELS)}")
    print(f"  âŒ å¤±è´¥: {len(failed_models)}")
    
    if failed_models:
        print(f"\nâŒ å¤±è´¥çš„æ¨¡å‹:")
        for model in failed_models:
            print(f"  - {model}")
    
    # éªŒè¯ç¼“å­˜ç»“æ„
    if verify_cache_structure():
        print(f"\nâœ… ç¼“å­˜éªŒè¯é€šè¿‡")
    else:
        print(f"\nâŒ ç¼“å­˜éªŒè¯å¤±è´¥")
        sys.exit(1)
    
    # åˆ›å»ºç¦»çº¿é…ç½®
    create_offline_config()
    
    # è®¾ç½®æƒé™
    os.system(f"chown -R 1000:1000 {cache_dir}")
    
    print(f"\nğŸ‰ æ‰€æœ‰åˆ†è¯å™¨é¢„ä¸‹è½½å®Œæˆï¼")
    print(f"ğŸ’¡ æç¤º: å®¹å™¨ç°åœ¨å¯ä»¥åœ¨ç¦»çº¿æ¨¡å¼ä¸‹è¿è¡Œ")
    
    if success_count < len(ALL_MODELS) * 0.8:  # å¦‚æœæˆåŠŸç‡ä½äº80%
        print(f"âš ï¸  è­¦å‘Š: æˆåŠŸç‡è¾ƒä½ ({success_count/len(ALL_MODELS)*100:.1f}%)")
        print(f"   è¿™å¯èƒ½ä¼šå½±å“æŸäº›æ¨¡å‹çš„ä½¿ç”¨")

if __name__ == "__main__":
    main()
