#!/usr/bin/env python3
"""
åˆ†è¯å™¨ç®¡ç†å·¥å…·
ç”¨äºæ›´æ–°ã€éªŒè¯å’Œç®¡ç†æœ¬åœ°åˆ†è¯å™¨ç¼“å­˜
"""

import os
import sys
import json
import argparse
import time
from transformers import AutoTokenizer
from huggingface_hub import snapshot_download

# ç¼“å­˜ç›®å½•
CACHE_DIR = "/data/cache"
CONFIG_FILE = os.path.join(CACHE_DIR, "offline_config.json")

def load_config():
    """åŠ è½½ç¦»çº¿é…ç½®"""
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"downloaded_models": [], "download_timestamp": None}

def save_config(config):
    """ä¿å­˜ç¦»çº¿é…ç½®"""
    config["download_timestamp"] = time.time()
    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

def list_models():
    """åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹"""
    config = load_config()
    print(f"ğŸ“¦ å·²ä¸‹è½½çš„æ¨¡å‹ ({len(config['downloaded_models'])} ä¸ª):")
    
    for model in sorted(config['downloaded_models']):
        model_path = os.path.join(CACHE_DIR, f"models--{model.replace('/', '--')}")
        if os.path.exists(model_path):
            size = get_dir_size(model_path)
            print(f"  âœ… {model} ({format_size(size)})")
        else:
            print(f"  âŒ {model} (ç¼“å­˜ä¸¢å¤±)")
    
    if config.get("download_timestamp"):
        download_time = time.strftime("%Y-%m-%d %H:%M:%S", 
                                    time.localtime(config["download_timestamp"]))
        print(f"\nğŸ•’ æœ€åæ›´æ–°æ—¶é—´: {download_time}")

def get_dir_size(path):
    """è·å–ç›®å½•å¤§å°"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.exists(filepath):
                total_size += os.path.getsize(filepath)
    return total_size

def format_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes == 0:
        return "0B"
    size_names = ["B", "KB", "MB", "GB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    return f"{size_bytes:.1f}{size_names[i]}"

def update_model(model_name):
    """æ›´æ–°æŒ‡å®šæ¨¡å‹çš„åˆ†è¯å™¨"""
    print(f"ğŸ”„ æ›´æ–°æ¨¡å‹: {model_name}")
    
    try:
        # å¼ºåˆ¶é‡æ–°ä¸‹è½½
        snapshot_download(
            repo_id=model_name,
            cache_dir=CACHE_DIR,
            local_files_only=False,
            resume_download=True,
            force_download=True,  # å¼ºåˆ¶é‡æ–°ä¸‹è½½
            ignore_patterns=["*.bin", "*.safetensors", "pytorch_model.bin"]
        )
        
        # éªŒè¯åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=CACHE_DIR,
            trust_remote_code=True,
            local_files_only=False,
            force_download=True
        )
        
        # æµ‹è¯•åˆ†è¯å™¨
        test_text = "Hello, world! ä½ å¥½ä¸–ç•Œï¼"
        tokens = tokenizer.encode(test_text)
        decoded = tokenizer.decode(tokens)
        
        print(f"  âœ… æ›´æ–°æˆåŠŸ: {model_name}")
        print(f"  ğŸ”¤ æµ‹è¯•é€šè¿‡: '{test_text}' -> {len(tokens)} tokens")
        
        # æ›´æ–°é…ç½®
        config = load_config()
        if model_name not in config["downloaded_models"]:
            config["downloaded_models"].append(model_name)
        save_config(config)
        
        return True
        
    except Exception as e:
        print(f"  âŒ æ›´æ–°å¤±è´¥: {e}")
        return False

def update_all():
    """æ›´æ–°æ‰€æœ‰å·²ä¸‹è½½çš„æ¨¡å‹"""
    config = load_config()
    models = config.get("downloaded_models", [])
    
    if not models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å·²ä¸‹è½½çš„æ¨¡å‹")
        return
    
    print(f"ğŸ”„ å¼€å§‹æ›´æ–° {len(models)} ä¸ªæ¨¡å‹...")
    
    success_count = 0
    for i, model in enumerate(models, 1):
        print(f"\n[{i}/{len(models)}] æ›´æ–°: {model}")
        if update_model(model):
            success_count += 1
    
    print(f"\nâœ… æ›´æ–°å®Œæˆ: {success_count}/{len(models)} æˆåŠŸ")

def verify_cache():
    """éªŒè¯ç¼“å­˜å®Œæ•´æ€§"""
    print("ğŸ” éªŒè¯ç¼“å­˜å®Œæ•´æ€§...")
    
    config = load_config()
    models = config.get("downloaded_models", [])
    
    valid_count = 0
    invalid_models = []
    
    for model in models:
        model_path = os.path.join(CACHE_DIR, f"models--{model.replace('/', '--')}")
        
        if not os.path.exists(model_path):
            print(f"  âŒ ç¼“å­˜ä¸¢å¤±: {model}")
            invalid_models.append(model)
            continue
        
        try:
            # å°è¯•åŠ è½½åˆ†è¯å™¨
            tokenizer = AutoTokenizer.from_pretrained(
                model,
                cache_dir=CACHE_DIR,
                local_files_only=True,
                trust_remote_code=True
            )
            
            # ç®€å•æµ‹è¯•
            tokens = tokenizer.encode("test")
            if len(tokens) > 0:
                print(f"  âœ… éªŒè¯é€šè¿‡: {model}")
                valid_count += 1
            else:
                print(f"  âŒ åˆ†è¯å™¨å¼‚å¸¸: {model}")
                invalid_models.append(model)
                
        except Exception as e:
            print(f"  âŒ éªŒè¯å¤±è´¥: {model} - {e}")
            invalid_models.append(model)
    
    print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
    print(f"  âœ… æœ‰æ•ˆ: {valid_count}/{len(models)}")
    print(f"  âŒ æ— æ•ˆ: {len(invalid_models)}")
    
    if invalid_models:
        print(f"\nâŒ éœ€è¦ä¿®å¤çš„æ¨¡å‹:")
        for model in invalid_models:
            print(f"  - {model}")
        
        if input("\nğŸ”§ æ˜¯å¦ä¿®å¤æ— æ•ˆçš„æ¨¡å‹? (y/N): ").lower() == 'y':
            for model in invalid_models:
                update_model(model)

def clean_cache():
    """æ¸…ç†ç¼“å­˜"""
    print("ğŸ§¹ æ¸…ç†ç¼“å­˜...")
    
    if input("âš ï¸  ç¡®å®šè¦æ¸…ç†æ‰€æœ‰ç¼“å­˜å—? è¿™å°†åˆ é™¤æ‰€æœ‰ä¸‹è½½çš„æ¨¡å‹ (y/N): ").lower() != 'y':
        print("âŒ å–æ¶ˆæ¸…ç†")
        return
    
    import shutil
    
    try:
        # å¤‡ä»½é…ç½®æ–‡ä»¶
        config = load_config()
        
        # æ¸…ç†ç¼“å­˜ç›®å½•
        for item in os.listdir(CACHE_DIR):
            item_path = os.path.join(CACHE_DIR, item)
            if item != "offline_config.json":
                if os.path.isdir(item_path):
                    shutil.rmtree(item_path)
                else:
                    os.remove(item_path)
        
        # é‡ç½®é…ç½®
        config["downloaded_models"] = []
        save_config(config)
        
        print("âœ… ç¼“å­˜æ¸…ç†å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ¸…ç†å¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(description="åˆ†è¯å™¨ç®¡ç†å·¥å…·")
    parser.add_argument("command", choices=["list", "update", "update-all", "verify", "clean"],
                       help="æ‰§è¡Œçš„å‘½ä»¤")
    parser.add_argument("--model", help="æŒ‡å®šæ¨¡å‹åç§° (ç”¨äº update å‘½ä»¤)")
    
    args = parser.parse_args()
    
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs(CACHE_DIR, exist_ok=True)
    
    if args.command == "list":
        list_models()
    elif args.command == "update":
        if not args.model:
            print("âŒ è¯·æŒ‡å®šæ¨¡å‹åç§°: --model MODEL_NAME")
            sys.exit(1)
        update_model(args.model)
    elif args.command == "update-all":
        update_all()
    elif args.command == "verify":
        verify_cache()
    elif args.command == "clean":
        clean_cache()

if __name__ == "__main__":
    main()
