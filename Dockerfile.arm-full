# ARMæž¶æž„å®Œæ•´ç‰ˆDockerfile
# åŒ…å«GPT-3.5ç­‰æ‰€æœ‰ä¸»æµæ¨¡åž‹çš„åˆ†è¯å™¨

# å‰ç«¯æž„å»ºé˜¶æ®µ
FROM --platform=$BUILDPLATFORM node:18-alpine AS builder

WORKDIR /build
COPY web/package.json web/pnpm-lock.yaml ./
RUN npm install -g pnpm && pnpm install
COPY ./web .
COPY ./VERSION .
RUN DISABLE_ESLINT_PLUGIN='true' VITE_REACT_APP_VERSION=$(cat VERSION) pnpm run build

# åŽç«¯æž„å»ºé˜¶æ®µ
FROM --platform=$BUILDPLATFORM golang:alpine AS builder2

# æž„å»ºå‚æ•°
ARG TARGETOS
ARG TARGETARCH

ENV GO111MODULE=on \
    CGO_ENABLED=0 \
    GOOS=$TARGETOS \
    GOARCH=$TARGETARCH

WORKDIR /build

ADD go.mod go.sum ./
RUN go mod download

COPY . .
COPY --from=builder /build/dist ./web/dist
RUN go build -ldflags "-s -w -X 'one-api/common.Version=$(cat VERSION)'" -o one-api

# åˆ†è¯å™¨é¢„ä¸‹è½½é˜¶æ®µï¼ˆä½¿ç”¨AMD64è¿›è¡Œé¢„ä¸‹è½½ï¼Œç„¶åŽå¤åˆ¶åˆ°ARMé•œåƒï¼‰
FROM --platform=linux/amd64 python:3.9-alpine AS tokenizer-downloader

# è®¾ç½®çŽ¯å¢ƒå˜é‡
ENV HUGGINGFACE_HUB_CACHE=/cache
ENV TRANSFORMERS_CACHE=/cache
ENV HF_HOME=/cache
ENV TIKTOKEN_CACHE_DIR=/cache/tiktoken

# å®‰è£…å¿…è¦çš„åŒ…
RUN apk add --no-cache gcc musl-dev libffi-dev
RUN pip install --no-cache-dir huggingface_hub transformers tiktoken

# åˆ›å»ºç¼“å­˜ç›®å½•
RUN mkdir -p /cache /cache/tiktoken

# å¤åˆ¶åˆ†è¯å™¨é¢„ä¸‹è½½è„šæœ¬
COPY docker/huggingface-tei/preload_gpt_tokenizers.py /preload_gpt_tokenizers.py
COPY docker/huggingface-tei/preload_tokenizers.py /preload_tokenizers.py

# è¿è¡Œåˆ†è¯å™¨é¢„ä¸‹è½½
RUN echo "å¼€å§‹é¢„ä¸‹è½½GPTç­‰ä¸»æµæ¨¡åž‹åˆ†è¯å™¨..." && \
    python /preload_gpt_tokenizers.py && \
    echo "å¼€å§‹é¢„ä¸‹è½½å…¶ä»–åˆ†è¯å™¨..." && \
    python /preload_tokenizers.py && \
    echo "åˆ†è¯å™¨é¢„ä¸‹è½½å®Œæˆ"

# éªŒè¯åˆ†è¯å™¨
RUN echo "éªŒè¯åˆ†è¯å™¨..." && \
    python -c "import tiktoken; print('âœ“ tiktokenå¯ç”¨'); enc = tiktoken.get_encoding('cl100k_base'); tokens = enc.encode('Hello GPT-3.5!'); print(f'âœ“ GPT-3.5åˆ†è¯å™¨æµ‹è¯•: {len(tokens)} tokens')" && \
    python -c "from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained('gpt2', cache_dir='/cache', trust_remote_code=True); print('âœ“ GPT-2åˆ†è¯å™¨å¯ç”¨')" || echo "âš  GPT-2åˆ†è¯å™¨åŠ è½½å¤±è´¥" && \
    echo "åˆ†è¯å™¨éªŒè¯å®Œæˆ"

# æœ€ç»ˆè¿è¡Œé˜¶æ®µ
FROM alpine

# æž„å»ºå‚æ•°
ARG TARGETPLATFORM

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apk update \
    && apk upgrade \
    && apk add --no-cache ca-certificates tzdata ffmpeg python3 py3-pip gcc musl-dev libffi-dev \
    && update-ca-certificates

# å¤åˆ¶é¢„ä¸‹è½½çš„åˆ†è¯å™¨ç¼“å­˜
COPY --from=tokenizer-downloader /cache /data/cache

# å®‰è£…Pythonä¾èµ–ç”¨äºŽè¿è¡Œæ—¶åˆ†è¯å™¨ç®¡ç†ï¼ˆè·³è¿‡tiktokenï¼Œå·²åœ¨é¢„ä¸‹è½½é˜¶æ®µå®‰è£…ï¼‰
RUN pip3 install --no-cache-dir --break-system-packages huggingface_hub transformers

# å¤åˆ¶åˆ†è¯å™¨ç®¡ç†è„šæœ¬
COPY docker/huggingface-tei/tokenizer_manager.py /usr/local/bin/tokenizer_manager.py
RUN chmod +x /usr/local/bin/tokenizer_manager.py

# å¤åˆ¶åº”ç”¨ç¨‹åº
COPY --from=builder2 /build/one-api /

# è®¾ç½®çŽ¯å¢ƒå˜é‡
ENV HUGGINGFACE_HUB_CACHE=/data/cache
ENV TRANSFORMERS_CACHE=/data/cache
ENV HF_HOME=/data/cache
ENV TIKTOKEN_CACHE_DIR=/data/cache/tiktoken

# åˆ›å»ºå¯åŠ¨æ—¶åˆ†è¯å™¨éªŒè¯è„šæœ¬
RUN echo '#!/usr/bin/env python3' > /verify_tokenizers.py && \
    echo 'import os' >> /verify_tokenizers.py && \
    echo 'import sys' >> /verify_tokenizers.py && \
    echo '' >> /verify_tokenizers.py && \
    echo 'def verify_tokenizers():' >> /verify_tokenizers.py && \
    echo '    print("ðŸ” éªŒè¯åˆ†è¯å™¨...")' >> /verify_tokenizers.py && \
    echo '    try:' >> /verify_tokenizers.py && \
    echo '        import tiktoken' >> /verify_tokenizers.py && \
    echo '        enc = tiktoken.get_encoding("cl100k_base")' >> /verify_tokenizers.py && \
    echo '        tokens = enc.encode("Hello GPT-3.5!")' >> /verify_tokenizers.py && \
    echo '        print(f"âœ“ GPT-3.5/GPT-4åˆ†è¯å™¨å¯ç”¨: {len(tokens)} tokens")' >> /verify_tokenizers.py && \
    echo '    except Exception as e:' >> /verify_tokenizers.py && \
    echo '        print(f"âœ— GPT-3.5/GPT-4åˆ†è¯å™¨ä¸å¯ç”¨: {e}")' >> /verify_tokenizers.py && \
    echo '    try:' >> /verify_tokenizers.py && \
    echo '        from transformers import AutoTokenizer' >> /verify_tokenizers.py && \
    echo '        tokenizer = AutoTokenizer.from_pretrained("gpt2", cache_dir="/data/cache", local_files_only=True)' >> /verify_tokenizers.py && \
    echo '        tokens = tokenizer.encode("Hello GPT-2!")' >> /verify_tokenizers.py && \
    echo '        print(f"âœ“ GPT-2åˆ†è¯å™¨å¯ç”¨: {len(tokens)} tokens")' >> /verify_tokenizers.py && \
    echo '    except Exception as e:' >> /verify_tokenizers.py && \
    echo '        print(f"âœ— GPT-2åˆ†è¯å™¨ä¸å¯ç”¨: {e}")' >> /verify_tokenizers.py && \
    echo '    print("åˆ†è¯å™¨éªŒè¯å®Œæˆ")' >> /verify_tokenizers.py && \
    echo '' >> /verify_tokenizers.py && \
    echo 'if __name__ == "__main__":' >> /verify_tokenizers.py && \
    echo '    verify_tokenizers()' >> /verify_tokenizers.py

RUN chmod +x /verify_tokenizers.py

# æ·»åŠ æž¶æž„ä¿¡æ¯æ ‡ç­¾
LABEL org.opencontainers.image.title="New API Self-Use Mode (ARM Full)"
LABEL org.opencontainers.image.description="Self-use mode with complete tokenizers for GPT-3.5, GPT-4, etc."
LABEL org.opencontainers.image.version="v1.0.0-self-use"
LABEL org.opencontainers.image.platform="$TARGETPLATFORM"

EXPOSE 3000
WORKDIR /data

# å¯åŠ¨æ—¶éªŒè¯åˆ†è¯å™¨
RUN echo '#!/bin/sh' > /entrypoint.sh && \
    echo 'echo "ðŸš€ å¯åŠ¨New APIè‡ªç”¨æ¨¡å¼ (ARMç‰ˆæœ¬)"' >> /entrypoint.sh && \
    echo 'python3 /verify_tokenizers.py' >> /entrypoint.sh && \
    echo 'exec /one-api "$@"' >> /entrypoint.sh && \
    chmod +x /entrypoint.sh && \
    chmod +x /verify_tokenizers.py

ENTRYPOINT ["/entrypoint.sh"]
